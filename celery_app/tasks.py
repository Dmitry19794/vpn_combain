#!/usr/bin/env python3
# vpn/celery_app/tasks.py
import os
import sys
from celery.schedules import crontab

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import signal
import subprocess
import psycopg2
import time
import traceback as tb
import tempfile
import ipaddress
from psycopg2.extras import RealDictCursor, execute_values
from celery import Celery, Task
from datetime import datetime
from typing import Dict, List, Optional

from web.db import get_db, db_pool

# ============================================
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Celery
# ============================================
app = Celery(
    'tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    broker_connection_retry_on_startup=True,
)

CHUNK_SIZE = 1000
CIDR_SPLIT_SIZE = 24  # /24 = 256 IP –≤ –±–ª–æ–∫–µ

# ============================================
# –õ–û–ì –û–®–ò–ë–û–ö
# ============================================
def save_worker_error(path: str, error: str, tb_text: Optional[str] = None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—à–∏–±–∫—É –≤ —Ç–∞–±–ª–∏—Ü—É app_errors"""
    conn = None
    try:
        conn = get_db()
        cur = conn.cursor()
        try:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS app_errors (
                    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    method TEXT,
                    path TEXT,
                    status_code INTEGER,
                    error TEXT,
                    traceback TEXT
                );
            """)
            cur.execute("""
                INSERT INTO app_errors (method, path, status_code, error, traceback)
                VALUES (%s, %s, %s, %s, %s)
            """, (
                "CELERY",
                path,
                500,
                (error or '')[:8000],
                (tb_text or tb.format_exc())[:16000]
            ))
            conn.commit()
        except Exception as e:
            try:
                conn.rollback()
            except:
                pass
            print("‚ùå FAILED TO INSERT app_errors row:", e)
    except Exception as e:
        print("‚ùå FAILED TO SAVE CELERY ERROR:", e)
    finally:
        if conn:
            try:
                db_pool.putconn(conn)
            except Exception:
                try:
                    conn.close()
                except:
                    pass

# ============================================
# –ö–û–ù–¢–†–û–õ–¨ –ó–ê–î–ê–ß
# ============================================
def get_job_control_status(job_id):
    conn = None
    try:
        conn = get_db()
        cur = conn.cursor()
        cur.execute("""
            SELECT control_action
            FROM scan_jobs
            WHERE id = %s
        """, (job_id,))
        row = cur.fetchone()
        return row[0] if row else None
    except Exception as e:
        print("‚ùå get_job_control_status error:", e)
        return None
    finally:
        if conn:
            db_pool.putconn(conn)

def update_job_progress(job_id: str, progress: float, conn):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç progress_percent"""
    try:
        cur = conn.cursor()
        cur.execute("""
            UPDATE scan_jobs
            SET progress_percent = %s
            WHERE id = %s
        """, (progress, job_id))
    except Exception as e:
        save_worker_error("update_job_progress", str(e), tb.format_exc())
        raise

# ============================================
# –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á (on_failure)
# ============================================
class MasscanTask(Task):
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        job_id = kwargs.get('job_id') or (args[0] if args else None)
        print(f"‚ùå Task {task_id} failed: {exc}")
        if not job_id:
            save_worker_error(path=f"task.on_failure:{task_id}", error=str(exc), tb_text=tb.format_exc())
            return

        conn = None
        try:
            conn = get_db()
            cur = conn.cursor()
            try:
                cur.execute("""
                    UPDATE scan_jobs
                    SET status = 'failed',
                        finished_at = NOW(),
                        process_pid = NULL,
                        control_action = NULL
                    WHERE id = %s
                """, (job_id,))
                conn.commit()
            except Exception as e:
                try:
                    conn.rollback()
                except:
                    pass
                print("Failed to update job status in on_failure:", e)
        except Exception as e:
            print("Failed to get connection in on_failure:", e)
        finally:
            if conn:
                try:
                    db_pool.putconn(conn)
                except:
                    try:
                        conn.close()
                    except:
                        pass

        save_worker_error(path=f"task.on_failure:{task_id}", error=str(exc), tb_text=tb.format_exc())

# ============================================
# –†–ê–ë–û–¢–ê –° –ü–†–û–ö–°–ò
# ============================================
def get_random_proxy(geo: str = None):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –∂–∏–≤–æ–π –ø—Ä–æ–∫—Å–∏ –∏–∑ –ë–î"""
    conn = None
    try:
        conn = get_db()
        cur = conn.cursor()
        
        if geo:
            cur.execute("""
                SELECT host, port
                FROM proxies
                WHERE is_alive = TRUE AND geo = %s
                ORDER BY RANDOM()
                LIMIT 1
            """, (geo,))
        else:
            cur.execute("""
                SELECT host, port
                FROM proxies
                WHERE is_alive = TRUE
                ORDER BY RANDOM()
                LIMIT 1
            """)
        
        row = cur.fetchone()
        if row:
            return (row[0], row[1])
        return None
        
    except Exception as e:
        print(f"‚ùå get_random_proxy error: {e}")
        return None
    finally:
        if conn:
            try:
                db_pool.putconn(conn)
            except:
                pass

def run_nmap_via_proxy(target: str, port: int, proxy_host: str, proxy_port: int, 
                       output_file: str, timeout: int = 120) -> bool:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç nmap —á–µ—Ä–µ–∑ proxychains"""
    config_path = None
    try:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ proxychains
        with tempfile.NamedTemporaryFile(mode='w', suffix='.conf', delete=False) as f:
            config_path = f.name
            f.write(f"""# Proxychains config
strict_chain
proxy_dns
tcp_read_time_out 15000
tcp_connect_time_out 8000

[ProxyList]
socks5 {proxy_host} {proxy_port}
""")
        
        print(f"üìù Created proxychains config: {proxy_host}:{proxy_port}")
        
        # –ö–æ–º–∞–Ω–¥–∞ nmap
        cmd = [
            'proxychains4', '-f', config_path, '-q',
            'nmap',
            '-p', str(port),
            '-sT',
            '-Pn',
            '--open',
            '-T4',
            '--max-retries', '1',
            '--host-timeout', '30s',
            '-oG', output_file,
            target
        ]
        
        print(f"üîç Running nmap via proxy {proxy_host}:{proxy_port} for {target}:{port}")
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        try:
            stdout, stderr = process.communicate(timeout=timeout)
            returncode = process.returncode
            
            if returncode == 0:
                print(f"‚úÖ Nmap completed for {target}")
                return True
            else:
                print(f"‚ö†Ô∏è Nmap returned code {returncode} for {target}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"‚è∞ Nmap timeout for {target}")
            process.kill()
            return False
            
    except Exception as e:
        print(f"‚ùå run_nmap_via_proxy error: {e}")
        return False
        
    finally:
        if config_path and os.path.exists(config_path):
            try:
                os.remove(config_path)
            except:
                pass

def split_cidr_into_blocks(cidr: str, block_size: int = 24) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç CIDR –Ω–∞ –±–ª–æ–∫–∏"""
    try:
        network = ipaddress.ip_network(cidr, strict=False)
        
        if network.prefixlen >= block_size:
            return [str(network)]
        
        subnets = list(network.subnets(new_prefix=block_size))
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
        if len(subnets) > 1000:
            print(f"‚ö†Ô∏è Too many subnets ({len(subnets)}), taking first 1000")
            subnets = subnets[:1000]
        
        return [str(subnet) for subnet in subnets]
        
    except Exception as e:
        print(f"‚ùå split_cidr error: {e}")
        return [cidr]

def parse_nmap_results(output_file: str, job_id: str, port: int, geo: str, conn) -> int:
    """–ü–∞—Ä—Å–∏—Ç greppable output nmap"""
    total = 0
    if not os.path.exists(output_file):
        return 0

    try:
        with open(output_file, 'r') as f:
            ips = []
            for line in f:
                if 'Host:' in line and 'Ports:' in line:
                    try:
                        parts = line.split()
                        host_idx = parts.index('Host:')
                        ip = parts[host_idx + 1]
                        
                        if '/open/' in line:
                            ips.append(ip)
                            
                    except Exception:
                        continue
                    
                    if len(ips) >= CHUNK_SIZE:
                        try:
                            count = insert_addresses_batch(conn, ips, port, geo, job_id)
                            conn.commit()
                            total += count
                        except Exception as e:
                            try:
                                conn.rollback()
                            except:
                                pass
                            print(f"‚ùå Batch insert error: {e}")
                        ips = []
            
            if ips:
                try:
                    count = insert_addresses_batch(conn, ips, port, geo, job_id)
                    conn.commit()
                    total += count
                except Exception as e:
                    try:
                        conn.rollback()
                    except:
                        pass
                    print(f"‚ùå Final insert error: {e}")
                    
    except Exception as e:
        print(f"‚ùå parse_nmap_results error: {e}")
    
    return total

# ============================================
# –û–°–ù–û–í–ù–ê–Ø –ó–ê–î–ê–ß–ê –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø
# ============================================
@app.task(bind=True, name='tasks.run_masscan', base=MasscanTask)
def run_masscan(self, job_id: str, cidr: str, port: int, geo: str):
    """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
    print(f"üîç Starting distributed scan: {cidr}:{port} (Job={job_id}, GEO={geo})")
    
    start_time = time.time()
    total_results = 0
    conn = None
    
    try:
        # –ü–æ–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ running
        try:
            conn = get_db()
            cur = conn.cursor()
            cur.execute("""
                UPDATE scan_jobs
                SET status = 'running',
                    started_at = NOW(),
                    assigned_to = %s,
                    progress_percent = 0,
                    control_action = NULL
                WHERE id = %s
            """, (self.request.id, job_id))
            conn.commit()
        except Exception as e:
            print(f"‚ùå Error setting running status: {e}")
        finally:
            if conn:
                try:
                    db_pool.putconn(conn)
                except:
                    pass
            conn = None
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏
        blocks = split_cidr_into_blocks(cidr, CIDR_SPLIT_SIZE)
        total_blocks = len(blocks)
        print(f"üìä Split {cidr} into {total_blocks} blocks (/{CIDR_SPLIT_SIZE})")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
        if total_blocks > 100:
            print(f"‚ö†Ô∏è Too many blocks ({total_blocks}), limiting to 100")
            blocks = blocks[:100]
            total_blocks = 100
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫
        for i, block in enumerate(blocks):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º control actions
            try:
                conn = get_db()
                cur = conn.cursor()
                cur.execute("SELECT control_action FROM scan_jobs WHERE id = %s", (job_id,))
                row = cur.fetchone()
                ctrl = row[0] if row else None
            except Exception as e:
                print(f"‚ö†Ô∏è Error checking control: {e}")
                ctrl = None
            finally:
                if conn:
                    try:
                        db_pool.putconn(conn)
                    except:
                        pass
                conn = None
            
            if ctrl == 'stop':
                print(f"üõë Stopping job {job_id}")
                break
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏
            proxy = get_random_proxy(geo)
            if not proxy:
                print(f"‚ö†Ô∏è No proxy for GEO={geo}, skipping {block}")
                continue
            
            proxy_host, proxy_port = proxy
            output_file = f"/tmp/nmap_{job_id}_{i}.txt"
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º nmap
            try:
                success = run_nmap_via_proxy(
                    target=block,
                    port=port,
                    proxy_host=proxy_host,
                    proxy_port=proxy_port,
                    output_file=output_file,
                    timeout=120
                )
            except Exception as e:
                print(f"‚ùå Nmap error for {block}: {e}")
                success = False
            
            if success:
                # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                try:
                    conn = get_db()
                    count = parse_nmap_results(output_file, job_id, port, geo, conn)
                    conn.commit()
                    total_results += count
                    print(f"‚úÖ Block {i+1}/{total_blocks}: found {count} hosts")
                except Exception as e:
                    print(f"‚ùå Parse error for block {i}: {e}")
                    if conn:
                        try:
                            conn.rollback()
                        except:
                            pass
                finally:
                    if conn:
                        try:
                            db_pool.putconn(conn)
                        except:
                            pass
                    conn = None
            else:
                print(f"‚ö†Ô∏è Block {i+1}/{total_blocks}: scan failed")
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                if os.path.exists(output_file):
                    os.remove(output_file)
            except:
                pass
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            progress = ((i + 1) / total_blocks) * 100
            try:
                conn = get_db()
                cur = conn.cursor()
                cur.execute("""
                    UPDATE scan_jobs
                    SET progress_percent = %s,
                        result_count = %s
                    WHERE id = %s
                """, (progress, total_results, job_id))
                conn.commit()
            except Exception as e:
                print(f"‚ö†Ô∏è Progress update error: {e}")
            finally:
                if conn:
                    try:
                        db_pool.putconn(conn)
                    except:
                        pass
                conn = None
            
            # Celery meta
            try:
                elapsed = time.time() - start_time
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'job_id': job_id,
                        'cidr': cidr,
                        'progress': progress,
                        'blocks_done': i + 1,
                        'blocks_total': total_blocks,
                        'results': total_results,
                        'elapsed': int(elapsed)
                    }
                )
            except:
                pass
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–ø–¥–µ–π—Ç
        elapsed = time.time() - start_time
        try:
            conn = get_db()
            cur = conn.cursor()
            cur.execute("""
                UPDATE scan_jobs
                SET status = 'completed',
                    finished_at = NOW(),
                    result_count = %s,
                    progress_percent = 100,
                    process_pid = NULL,
                    control_action = NULL
                WHERE id = %s
            """, (total_results, job_id))
            conn.commit()
        except Exception as e:
            print(f"‚ùå Final update error: {e}")
        finally:
            if conn:
                try:
                    db_pool.putconn(conn)
                except:
                    pass
        
        print(f"‚úÖ Job {job_id} completed: {total_results} hosts in {int(elapsed)}s")
        return {
            'status': 'completed',
            'job_id': job_id,
            'cidr': cidr,
            'port': port,
            'geo': geo,
            'result_count': total_results,
            'elapsed_seconds': int(elapsed),
            'blocks_scanned': total_blocks
        }
        
    except Exception as e:
        print(f"‚ùå Fatal error in run_masscan: {e}")
        import traceback
        traceback.print_exc()
        
        # –ü–æ–º–µ—á–∞–µ–º failed
        try:
            conn = get_db()
            cur = conn.cursor()
            cur.execute("""
                UPDATE scan_jobs
                SET status = 'failed',
                    finished_at = NOW(),
                    result_count = %s,
                    process_pid = NULL,
                    control_action = NULL
                WHERE id = %s
            """, (total_results, job_id))
            conn.commit()
        except:
            pass
        finally:
            if conn:
                try:
                    db_pool.putconn(conn)
                except:
                    pass
        
        raise

# ============================================
# –í–°–¢–ê–í–ö–ê –ê–î–†–ï–°–û–í
# ============================================
def insert_addresses_batch(conn, ips: List[str], port: int, geo: str, job_id: str) -> int:
    if not ips:
        return 0

    cur = conn.cursor()
    rows = [(ip, port, geo, job_id) for ip in ips]

    sql = """
        INSERT INTO scanned_addresses (id, ip, port, geo, job_id, is_checked, created_at, updated_at)
        SELECT gen_random_uuid(), data.ip::inet, data.port, data.geo, data.job_id::uuid, FALSE, NOW(), NOW()
        FROM (VALUES %s) AS data(ip, port, geo, job_id)
        ON CONFLICT DO NOTHING
    """

    try:
        execute_values(cur, sql, rows)
        return len(rows)
    except Exception as e:
        save_worker_error("insert_addresses_batch", str(e), tb.format_exc())
        return 0

# ============================================
# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∞–º–∏ (pause/stop/resume)
# ============================================
@app.task(name='tasks.control_job')
def control_job(job_id: str, action: str):
    if action not in ['pause', 'resume', 'stop']:
        return {'error': f'Invalid action: {action}'}

    conn = None
    try:
        conn = get_db()
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("""
            UPDATE scan_jobs
            SET control_action = %s,
                control_updated_at = NOW()
            WHERE id = %s
            RETURNING status
        """, (action, job_id))
        row = cur.fetchone()
        if not row:
            try:
                conn.rollback()
            except:
                pass
            return {'error': 'Job not found'}
        conn.commit()
        print(f"üéõÔ∏è Job {job_id}: action={action}")
        result = {
            'status': 'success',
            'job_id': job_id,
            'action': action,
            'current_status': row.get('status') if isinstance(row, dict) else row[0]
        }
        return result
    except Exception as e:
        print(f"‚ùå Error controlling job: {e}")
        try:
            if conn:
                conn.rollback()
        except:
            pass
        save_worker_error(path=f"control_job:{job_id}", error=str(e), tb_text=tb.format_exc())
        return {'error': str(e)}
    finally:
        if conn:
            try:
                db_pool.putconn(conn)
            except:
                try:
                    conn.close()
                except:
                    pass

# ============================================
# –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# ============================================
@app.task(name='tasks.cleanup_old_data')
def cleanup_old_data(days: int = 7):
    conn = None
    try:
        conn = get_db()
        cur = conn.cursor()
        cur.execute("""
            DELETE FROM scan_jobs
            WHERE status IN ('completed', 'stopped', 'failed')
            AND finished_at < NOW() - INTERVAL '%s days'
        """, (days,))
        deleted_jobs = cur.rowcount

        cur.execute("""
            DELETE FROM scanned_addresses
            WHERE is_checked = TRUE 
            AND updated_at < NOW() - INTERVAL '%s days'
        """, (days,))
        deleted_addrs = cur.rowcount

        conn.commit()
        print(f"üßπ Cleaned: {deleted_jobs} jobs, {deleted_addrs} addresses")
        return {'status': 'success', 'deleted_jobs': deleted_jobs, 'deleted_addresses': deleted_addrs}
    except Exception as e:
        print(f"‚ùå Cleanup error: {e}")
        try:
            if conn:
                conn.rollback()
        except:
            pass
        save_worker_error(path=f"cleanup_old_data:{days}", error=str(e), tb_text=tb.format_exc())
        raise
    finally:
        if conn:
            try:
                db_pool.putconn(conn)
            except:
                try:
                    conn.close()
                except:
                    pass

# ============================================
# Beat schedule
# ============================================
app.conf.beat_schedule = {
    "auto-start-pending-us": {
        "task": "tasks.process_pending_scans",
        "schedule": 15.0,
        "args": ("US", 10),
    },
}

# ============================================
# –ê–í–¢–û–ó–ê–ü–£–°–ö PENDING
# ============================================
# –ù–ê–ô–î–ò –í tasks.py —Ñ—É–Ω–∫—Ü–∏—é process_pending_scans –∏ –ó–ê–ú–ï–ù–ò –Ω–∞ —ç—Ç–æ:

@app.task(name='tasks.process_pending_scans')
def process_pending_scans(geo: str = 'US', limit: int = 10):
    """–ë–µ—Ä—ë–º pending –∑–∞–¥–∞—á–∏ –∏ –∑–∞–ø—É—Å–∫–∞–µ–º"""
    conn = None
    try:
        conn = get_db()
        cur = conn.cursor()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        cur.execute("""
            SELECT j.id, s.cidr, p.port, s.geo
            FROM scan_jobs j
            JOIN scan_subnets s ON j.subnet_id = s.id
            JOIN scan_ports p ON j.port_id = p.id
            WHERE j.status = 'pending' AND s.geo = %s
            ORDER BY j.created_at ASC
            LIMIT %s
            FOR UPDATE SKIP LOCKED
        """, (geo, limit))
        
        # –ü–†–û–í–ï–†–Ø–ï–ú —á—Ç–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if cur.rowcount == 0:
            conn.commit()
            return None
            
        jobs = cur.fetchall()
        
        if not jobs:
            conn.commit()
            return None

        launched = []
        for job in jobs:
            jid = str(job[0])
            cidr = job[1]
            p = job[2]
            g = job[3]
            cur.execute("UPDATE scan_jobs SET status='queued' WHERE id=%s", (jid,))
            launched.append(jid)
            run_masscan.delay(jid, cidr, p, g)

        conn.commit()
        return {'status': 'launched', 'count': len(launched)}
        
    except Exception as e:
        print(f"‚ùå process_pending_scans error: {e}")
        try:
            if conn:
                conn.rollback()
        except:
            pass
        # –ù–ï –±—Ä–æ—Å–∞–µ–º exception, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None
    finally:
        if conn:
            try:
                db_pool.putconn(conn)
            except:
                try:
                    conn.close()
                except:
                    pass
